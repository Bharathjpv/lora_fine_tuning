{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - Tata Insights and Quants\\Work-Profile\\lora\\lora_fine_tuning\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "\n",
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "\n",
    "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0\n",
    "    \n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.1)\n",
    "    return dataset_splits.values()\n",
    "    \n",
    "\n",
    "def create_label_mappings(dataset):\n",
    "    label2id, id2label = dict(), dict()\n",
    "    for i, label in enumerate(dataset.features[\"label\"].names):\n",
    "        label2id[label] = i\n",
    "        id2label[i] = label \n",
    "\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This is the food dataset\n",
    "dataset1 = load_dataset(\"food101\", split=\"train[:10000]\")\n",
    "\n",
    "# This is the datasets of pictures of cats and dogs.\n",
    "# Notice we need to rename the label column so we can\n",
    "# reuse the same code for both datasets.\n",
    "# dataset2 = load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\", trust_remote_code=True)\n",
    "# dataset2 = dataset2.rename_column(\"labels\", \"label\")\n",
    "\n",
    "dataset1_train, dataset1_test = split_dataset(dataset1)\n",
    "# dataset2_train, dataset2_test = split_dataset(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_label2id, dataset1_id2label = create_label_mappings(dataset1)\n",
    "# dataset2_label2id, dataset2_id2label = create_label_mappings(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1:{\n",
    "        \"train_data\": dataset1_train,\n",
    "        \"test_data\": dataset1_test,\n",
    "        \"label2id\": dataset1_label2id,\n",
    "        \"id2label\": dataset1_id2label,\n",
    "        \"epochs\": 5,\n",
    "        \"path\": \"./lora-model1\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This image processor cannot be instantiated. Please make sure you have `Pillow` installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor\n\u001b[1;32m----> 3\u001b[0m image_processor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoImageProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive - Tata Insights and Quants\\Work-Profile\\lora\\lora_fine_tuning\\venv\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:481\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m image_processor_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis image processor cannot be instantiated. Please make sure you have `Pillow` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized image processor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Should have a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`image_processor_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mIMAGE_PROCESSOR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or one of the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model_type` keys in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mIMAGE_PROCESSOR_MAPPING_NAMES\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This image processor cannot be instantiated. Please make sure you have `Pillow` installed."
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This image processor cannot be instantiated. Please make sure you have `Pillow` installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor, AutoModel\n\u001b[1;32m----> 4\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoImageProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/vit-base-patch16-224-in21k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224-in21k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\OneDrive - Tata Insights and Quants\\Work-Profile\\lora\\lora_fine_tuning\\venv\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:481\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m image_processor_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis image processor cannot be instantiated. Please make sure you have `Pillow` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized image processor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Should have a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`image_processor_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mIMAGE_PROCESSOR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or one of the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model_type` keys in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mIMAGE_PROCESSOR_MAPPING_NAMES\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This image processor cannot be instantiated. Please make sure you have `Pillow` installed."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
