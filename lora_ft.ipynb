{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/vit-base-patch16-224-in21k'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jpv/Bharath_J_P_V/Personal_Projects/lora_fine_tuning/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "\n",
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "\n",
    "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0\n",
    "    \n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.1)\n",
    "    return dataset_splits.values()\n",
    "    \n",
    "\n",
    "def create_label_mappings(dataset):\n",
    "    label2id, id2label = dict(), dict()\n",
    "    for i, label in enumerate(dataset.features[\"label\"].names):\n",
    "        label2id[label] = i\n",
    "        id2label[i] = label \n",
    "\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 22.8MB/s]\n",
      "Downloading data: 100%|██████████| 490M/490M [00:43<00:00, 11.1MB/s] \n",
      "Downloading data: 100%|██████████| 464M/464M [00:38<00:00, 12.0MB/s] \n",
      "Downloading data: 100%|██████████| 472M/472M [00:39<00:00, 12.1MB/s] \n",
      "Downloading data: 100%|██████████| 464M/464M [00:38<00:00, 12.1MB/s] \n",
      "Downloading data: 100%|██████████| 475M/475M [00:43<00:00, 11.0MB/s] \n",
      "Downloading data: 100%|██████████| 470M/470M [00:42<00:00, 11.2MB/s] \n",
      "Downloading data: 100%|██████████| 478M/478M [00:39<00:00, 12.0MB/s] \n",
      "Downloading data: 100%|██████████| 486M/486M [00:40<00:00, 11.9MB/s] \n",
      "Downloading data: 100%|██████████| 423M/423M [00:34<00:00, 12.1MB/s] \n",
      "Downloading data: 100%|██████████| 413M/413M [00:38<00:00, 10.9MB/s] \n",
      "Downloading data: 100%|██████████| 426M/426M [00:35<00:00, 12.1MB/s] \n",
      "Generating train split: 100%|██████████| 75750/75750 [00:02<00:00, 27537.62 examples/s]\n",
      "Generating validation split: 100%|██████████| 25250/25250 [00:00<00:00, 28578.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This is the food dataset\n",
    "dataset1 = load_dataset(\"food101\", split=\"train[:10000]\")\n",
    "\n",
    "# This is the datasets of pictures of cats and dogs.\n",
    "# Notice we need to rename the label column so we can\n",
    "# reuse the same code for both datasets.\n",
    "# dataset2 = load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\", trust_remote_code=True)\n",
    "# dataset2 = dataset2.rename_column(\"labels\", \"label\")\n",
    "\n",
    "dataset1_train, dataset1_test = split_dataset(dataset1)\n",
    "# dataset2_train, dataset2_test = split_dataset(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_label2id, dataset1_id2label = create_label_mappings(dataset1)\n",
    "# dataset2_label2id, dataset2_id2label = create_label_mappings(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = {\n",
    "        \"train_data\": dataset1_train,\n",
    "        \"test_data\": dataset1_test,\n",
    "        \"label2id\": dataset1_label2id,\n",
    "        \"id2label\": dataset1_id2label,\n",
    "        \"epochs\": 5,\n",
    "        \"path\": \"./lora-model1\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "preprocess_pipeline = Compose([\n",
    "    Resize(image_processor.size[\"height\"]),\n",
    "    CenterCrop(image_processor.size[\"height\"]),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "def preprocess(batch):\n",
    "    batch[\"pixel_values\"] = [\n",
    "        preprocess_pipeline(image.convert(\"RGB\")) for image in batch[\"image\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1[\"train_data\"].set_transform(preprocess)\n",
    "model1[\"test_data\"].set_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def data_collate(examples):\n",
    "    \"\"\"\n",
    "    Prepare a batch of examples from a list of elements of the\n",
    "    train or test datasets.\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute the model's accuracy on a batch of predictions.\n",
    "    \"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "def get_base_model(label2id, id2label):\n",
    "    \"\"\"\n",
    "    Create an image classification base model from\n",
    "    the model checkpoint.\n",
    "    \"\"\"\n",
    "    return AutoModelForImageClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_lora_model(label2id, id2label):\n",
    "    \"\"\"Build the LoRA model to fine-tune the base model.\"\"\"\n",
    "    model = get_base_model(label2id, id2label)\n",
    "    print_trainable_parameters(model, label=\"Base model\")\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        modules_to_save=[\"classifier\"],\n",
    "    )\n",
    "\n",
    "    lora_model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(lora_model, label=\"LoRA\")\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./model-checkpoints\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    use_cpu=True,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model trainable parameters: 85,876,325/85,876,325 (100.00%)\n",
      "LoRA trainable parameters: 667,493/86,543,818 (0.77%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "training_arguments.num_train_epochs = model1[\"epochs\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    build_lora_model(model1[\"label2id\"], model1[\"id2label\"]),\n",
    "    training_arguments,\n",
    "    train_dataset=model1[\"train_data\"],\n",
    "    eval_dataset=model1[\"test_data\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "for cfg in config.values():\n",
    "    training_arguments.num_train_epochs = cfg[\"epochs\"]\n",
    "\n",
    "    trainer = Trainer(\n",
    "        build_lora_model(cfg[\"label2id\"], cfg[\"id2label\"]),\n",
    "        training_arguments,\n",
    "        train_dataset=cfg[\"train_data\"],\n",
    "        eval_dataset=cfg[\"test_data\"],\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collate,\n",
    "    )\n",
    "\n",
    "    results = trainer.train()\n",
    "    evaluation_results = trainer.evaluate(cfg['test_data'])\n",
    "    print(f\"Evaluation accuracy: {evaluation_results['eval_accuracy']}\")\n",
    "\n",
    "    # We can now save the fine-tuned model to disk.\n",
    "    trainer.save_model(cfg[\"path\"])\n",
    "    print_model_size(cfg[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 10/700 [01:22<1:32:34,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4884, 'grad_norm': 0.7463366389274597, 'learning_rate': 0.004928571428571429, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 20/700 [02:42<1:30:46,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4479, 'grad_norm': 0.3320975601673126, 'learning_rate': 0.004857142857142858, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 30/700 [04:03<1:29:51,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3172, 'grad_norm': 0.7150269746780396, 'learning_rate': 0.004785714285714286, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 40/700 [05:23<1:28:50,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2234, 'grad_norm': 0.6246882081031799, 'learning_rate': 0.004714285714285714, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 50/700 [06:44<1:26:29,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2972, 'grad_norm': 0.6688875555992126, 'learning_rate': 0.004642857142857143, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 60/700 [08:03<1:23:58,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2321, 'grad_norm': 0.6568211913108826, 'learning_rate': 0.004571428571428572, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 70/700 [09:23<1:22:24,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2576, 'grad_norm': 1.062619924545288, 'learning_rate': 0.0045000000000000005, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 80/700 [10:42<1:21:40,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2315, 'grad_norm': 0.6849544048309326, 'learning_rate': 0.004428571428571428, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 90/700 [12:02<1:20:38,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2553, 'grad_norm': 1.1308133602142334, 'learning_rate': 0.004357142857142857, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 100/700 [13:21<1:18:24,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2589, 'grad_norm': 0.44937172532081604, 'learning_rate': 0.004285714285714286, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 110/700 [14:40<1:18:23,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2284, 'grad_norm': 0.6282328963279724, 'learning_rate': 0.004214285714285715, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 120/700 [15:59<1:16:08,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2339, 'grad_norm': 0.9051182270050049, 'learning_rate': 0.0041428571428571434, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 130/700 [17:19<1:15:10,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3136, 'grad_norm': 1.1355319023132324, 'learning_rate': 0.004071428571428571, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 140/700 [18:37<1:13:31,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3191, 'grad_norm': 0.7820829153060913, 'learning_rate': 0.004, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 20%|██        | 140/700 [19:31<1:13:31,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25439518690109253, 'eval_accuracy': 0.924, 'eval_runtime': 48.1668, 'eval_samples_per_second': 20.761, 'eval_steps_per_second': 1.308, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 150/700 [20:48<1:22:13,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1766, 'grad_norm': 0.7268509864807129, 'learning_rate': 0.003928571428571429, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 160/700 [22:11<1:14:24,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1532, 'grad_norm': 0.17718355357646942, 'learning_rate': 0.0038571428571428576, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 170/700 [23:31<1:11:29,  8.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1621, 'grad_norm': 0.9461444616317749, 'learning_rate': 0.0037857142857142855, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 180/700 [24:51<1:09:50,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1494, 'grad_norm': 0.5911280512809753, 'learning_rate': 0.0037142857142857147, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 190/700 [26:14<1:09:02,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.148, 'grad_norm': 0.2960309386253357, 'learning_rate': 0.0036428571428571426, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 200/700 [27:33<1:06:54,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1304, 'grad_norm': 0.6738382577896118, 'learning_rate': 0.0035714285714285718, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 210/700 [28:56<1:06:46,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.133, 'grad_norm': 0.361554890871048, 'learning_rate': 0.0034999999999999996, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 220/700 [30:20<1:07:45,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1476, 'grad_norm': 0.5292849540710449, 'learning_rate': 0.003428571428571429, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 230/700 [31:42<1:04:29,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1382, 'grad_norm': 0.6103649139404297, 'learning_rate': 0.003357142857142857, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 240/700 [33:03<1:02:28,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2002, 'grad_norm': 0.49247828125953674, 'learning_rate': 0.003285714285714286, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 250/700 [34:24<1:00:03,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.111, 'grad_norm': 0.3469056189060211, 'learning_rate': 0.0032142857142857147, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 260/700 [35:47<1:01:44,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1803, 'grad_norm': 0.756784200668335, 'learning_rate': 0.003142857142857143, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 270/700 [37:08<58:36,  8.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1699, 'grad_norm': 0.2297605276107788, 'learning_rate': 0.0030714285714285717, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 280/700 [38:30<56:39,  8.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1413, 'grad_norm': 0.3629840612411499, 'learning_rate': 0.003, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 281/700 [39:33<56:19,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2305273860692978, 'eval_accuracy': 0.929, 'eval_runtime': 51.8342, 'eval_samples_per_second': 19.292, 'eval_steps_per_second': 1.215, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 290/700 [40:45<1:03:27,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0555, 'grad_norm': 0.3083096444606781, 'learning_rate': 0.002928571428571429, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 300/700 [42:08<55:56,  8.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0448, 'grad_norm': 0.20777277648448944, 'learning_rate': 0.002857142857142857, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 310/700 [43:34<57:34,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0516, 'grad_norm': 0.07422656565904617, 'learning_rate': 0.002785714285714286, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 320/700 [44:59<54:20,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0527, 'grad_norm': 0.40222257375717163, 'learning_rate': 0.0027142857142857142, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 330/700 [46:23<52:43,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0452, 'grad_norm': 0.47378888726234436, 'learning_rate': 0.002642857142857143, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 340/700 [47:46<48:47,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0538, 'grad_norm': 0.5559144616127014, 'learning_rate': 0.0025714285714285713, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 350/700 [49:01<43:40,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0507, 'grad_norm': 0.36676421761512756, 'learning_rate': 0.0025, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 360/700 [50:15<42:05,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0402, 'grad_norm': 0.41433799266815186, 'learning_rate': 0.002428571428571429, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 370/700 [51:30<40:47,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0504, 'grad_norm': 0.9752363562583923, 'learning_rate': 0.002357142857142857, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 380/700 [52:44<39:12,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0646, 'grad_norm': 0.9773350358009338, 'learning_rate': 0.002285714285714286, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 390/700 [53:58<38:05,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0579, 'grad_norm': 0.5188830494880676, 'learning_rate': 0.002214285714285714, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 400/700 [55:13<37:39,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0707, 'grad_norm': 0.6139543056488037, 'learning_rate': 0.002142857142857143, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 410/700 [56:29<36:53,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0516, 'grad_norm': 0.5116169452667236, 'learning_rate': 0.0020714285714285717, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 420/700 [57:48<36:14,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0521, 'grad_norm': 0.4523029029369354, 'learning_rate': 0.002, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 422/700 [58:49<35:57,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2218172401189804, 'eval_accuracy': 0.937, 'eval_runtime': 45.3382, 'eval_samples_per_second': 22.056, 'eval_steps_per_second': 1.39, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 430/700 [1:12:59<9:23:05, 125.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0158, 'grad_norm': 0.12725329399108887, 'learning_rate': 0.0019285714285714288, 'epoch': 3.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 440/700 [1:14:20<48:50, 11.27s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.018, 'grad_norm': 0.0740182176232338, 'learning_rate': 0.0018571428571428573, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 450/700 [1:15:38<33:10,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0089, 'grad_norm': 0.0729297548532486, 'learning_rate': 0.0017857142857142859, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 460/700 [1:16:58<31:44,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0082, 'grad_norm': 0.05495526269078255, 'learning_rate': 0.0017142857142857144, 'epoch': 3.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 470/700 [1:18:17<30:19,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'grad_norm': 0.06741288304328918, 'learning_rate': 0.001642857142857143, 'epoch': 3.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 480/700 [1:19:38<29:43,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.01, 'grad_norm': 0.017896447330713272, 'learning_rate': 0.0015714285714285715, 'epoch': 3.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 490/700 [1:20:55<26:10,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0111, 'grad_norm': 0.037134576588869095, 'learning_rate': 0.0015, 'epoch': 3.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 500/700 [1:22:09<24:48,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0183, 'grad_norm': 0.033515818417072296, 'learning_rate': 0.0014285714285714286, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 510/700 [1:23:23<23:33,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0114, 'grad_norm': 0.14146484434604645, 'learning_rate': 0.0013571428571428571, 'epoch': 3.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 520/700 [1:24:38<22:24,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0078, 'grad_norm': 0.0054623158648610115, 'learning_rate': 0.0012857142857142856, 'epoch': 3.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 530/700 [1:25:55<21:58,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0103, 'grad_norm': 0.1460408866405487, 'learning_rate': 0.0012142857142857144, 'epoch': 3.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 540/700 [1:27:14<20:51,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0141, 'grad_norm': 0.0630146712064743, 'learning_rate': 0.001142857142857143, 'epoch': 3.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 550/700 [1:28:32<19:31,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0124, 'grad_norm': 0.41994595527648926, 'learning_rate': 0.0010714285714285715, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 560/700 [1:29:51<18:20,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0196, 'grad_norm': 0.08102837204933167, 'learning_rate': 0.001, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 563/700 [1:31:00<17:10,  7.52s/it]/Users/jpv/Bharath_J_P_V/Personal_Projects/lora_fine_tuning/venv/lib/python3.12/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 7a20b1ba-2029-4fd0-901c-330668f2dd7b)') - silently ignoring the lookup for the file config.json in google/vit-base-patch16-224-in21k.\n",
      "  warnings.warn(\n",
      "/Users/jpv/Bharath_J_P_V/Personal_Projects/lora_fine_tuning/venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in google/vit-base-patch16-224-in21k - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20818603038787842, 'eval_accuracy': 0.94, 'eval_runtime': 46.1359, 'eval_samples_per_second': 21.675, 'eval_steps_per_second': 1.366, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 570/700 [1:31:55<20:38,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0064, 'grad_norm': 0.05691816657781601, 'learning_rate': 0.0009285714285714287, 'epoch': 4.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 580/700 [1:33:15<15:52,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'grad_norm': 0.030334725975990295, 'learning_rate': 0.0008571428571428572, 'epoch': 4.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 590/700 [1:34:34<14:40,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'grad_norm': 0.005318993702530861, 'learning_rate': 0.0007857142857142857, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 600/700 [1:36:02<14:52,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0027, 'grad_norm': 0.014028681442141533, 'learning_rate': 0.0007142857142857143, 'epoch': 4.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 610/700 [1:37:30<13:18,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'grad_norm': 0.010969904251396656, 'learning_rate': 0.0006428571428571428, 'epoch': 4.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 620/700 [1:38:52<10:52,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0047, 'grad_norm': 0.07930019497871399, 'learning_rate': 0.0005714285714285715, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 630/700 [1:40:12<09:18,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'grad_norm': 0.021491754800081253, 'learning_rate': 0.0005, 'epoch': 4.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 640/700 [1:41:33<08:02,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0035, 'grad_norm': 0.06546765565872192, 'learning_rate': 0.0004285714285714286, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 650/700 [1:42:53<06:41,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'grad_norm': 0.005614639725536108, 'learning_rate': 0.00035714285714285714, 'epoch': 4.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 660/700 [1:44:15<05:30,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'grad_norm': 0.02387746423482895, 'learning_rate': 0.00028571428571428574, 'epoch': 4.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 670/700 [1:45:41<04:16,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'grad_norm': 0.03190445527434349, 'learning_rate': 0.0002142857142857143, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 680/700 [1:47:04<02:48,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0097, 'grad_norm': 0.016122493892908096, 'learning_rate': 0.00014285714285714287, 'epoch': 4.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 690/700 [1:48:28<01:25,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'grad_norm': 0.12231434881687164, 'learning_rate': 7.142857142857143e-05, 'epoch': 4.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [1:49:52<00:00,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0023, 'grad_norm': 0.03155376389622688, 'learning_rate': 0.0, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 700/700 [1:50:43<00:00,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20703613758087158, 'eval_accuracy': 0.946, 'eval_runtime': 50.3043, 'eval_samples_per_second': 19.879, 'eval_steps_per_second': 1.252, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [1:50:43<00:00,  9.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6643.6853, 'train_samples_per_second': 6.773, 'train_steps_per_second': 0.105, 'train_loss': 0.13161656854408127, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:48<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.946\n",
      "Model size: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "results = trainer.train()\n",
    "evaluation_results = trainer.evaluate(model1['test_data'])\n",
    "print(f\"Evaluation accuracy: {evaluation_results['eval_accuracy']}\")\n",
    "\n",
    "# We can now save the fine-tuned model to disk.\n",
    "trainer.save_model(model1[\"path\"])\n",
    "print_model_size(model1[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_model(label2id, id2label, lora_adapter_path):\n",
    "    \"\"\"Build the model that will be use to run inference.\"\"\"\n",
    "\n",
    "    # Let's load the base model\n",
    "    model = get_base_model(label2id, id2label)\n",
    "\n",
    "    # Now, we can create the inference model combining the base model\n",
    "    # with the fine-tuned LoRA adapter.\n",
    "    return PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "\n",
    "\n",
    "def predict(image, model, image_processor):\n",
    "    \"\"\"Predict the class represented by the supplied image.\"\"\"\n",
    "    \n",
    "    encoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    class_index = logits.argmax(-1).item()\n",
    "    return model.config.id2label[class_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model1[\"inference_model\"] = build_inference_model(model1[\"label2id\"], model1[\"id2label\"], model1[\"path\"]) \n",
    "model1[\"image_processor\"] = AutoImageProcessor.from_pretrained(model1[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {\n",
    "    'image1': \"https://www.allrecipes.com/thmb/AtViolcfVtInHgq_mRtv4tPZASQ=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/ALR-187822-baked-chicken-wings-4x3-5c7b4624c8554f3da5aabb7d3a91a209.jpg\",\n",
    "    'image2':  \"https://www.simplyrecipes.com/thmb/KE6iMblr3R2Db6oE8HdyVsFSj2A=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/__opt__aboutcom__coeus__resources__content_migration__simply_recipes__uploads__2019__09__easy-pepperoni-pizza-lead-3-1024x682-583b275444104ef189d693a64df625da.jpg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: pizza\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(requests.get(images[\"image2\"], stream=True).raw)\n",
    "\n",
    "inference_model = model1[\"inference_model\"]\n",
    "image_processor = model1[\"image_processor\"]\n",
    "\n",
    "prediction = predict(image, inference_model, image_processor)\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
